{
  "hash": "041fdbc08d3efbea25274772817026f0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Nonparametric Econometrics Ch1\"\nsubtitle: Ch1：abd\n\ndate: last-modified\n\nauthor: \"Jiang Kunyi\"\n\ncategories: \n  - Nonpara\n\nimage: images/hero_right.png\nanchor-sections: true\nsearch: false\ncode-annotations: select\ncss: styles.css\ntoc: true\nreference-location: margin\ncitation-location: margin\n#execute: \n # echo: fenced\n # enabled: false\n\nformat:\n  html:\n    self-contained: true\n    number-sections: true\n    code-fold: false\n    code-tools: true\n    grid:\n       margin-width: 350px\n  #pdf: default\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nradius <- 5\n```\n:::\n\n\nThe radius of the circle is 5\n\nThis document demonstrates the use of a number of advanced page layout features to produce an attractive and usable document inspired by the Tufte handout style and the use of Tufte's styles in RMarkdown documents. The Tufte handout style is a style that Edward Tufte uses in his books and handouts. Tufte's style is known for its extensive use of sidenotes, tight integration of graphics with text, and well-set typography. Quarto[^1] supports most of the layout techniques that are used in the Tufte handout style for both HTML and LaTeX/PDF output.\n\n[^1]: To learn more, you can read more about [Quarto](https://www.quarto.org) or visit [Quarto's Github repository](https://www.github.com/quarto-dev/quarto-cli).\n\n::: column-page-right\n*R is free software and comes with ABSOLUTELY NO WARRANTY.* You are welcome to redistribute it under the terms of the GNU General Public License versions 2 or 3. For more information about these matters see <https://www.gnu.org/licenses/>.\n:::\n\n``` yaml\n---\ntitle: \"An Example Using the Tufte Style\"\nauthor: \"John Smith\"\nformat:\n  html:\n    grid:\n      margin-width: 350px         # <1>\n  pdf: default\nreference-location: margin        # <2>\ncitation-location: margin         # <2>\n---\n```\n\n1.  Increases the width of the margin to make more room for sidenotes and margin figures (HTML only).\n2.  Places footnotes and cited sources in the margin. Other layout options (for example placing a figure in the margin) will be set per element in examples below.\n\n# One-Sample Methods\n\nThe nonparametric tests described in this course are often called distribution-free test procedures because the validity of the tests does not depend on the underlying model (distribution) assumption. In other words, the significance level of the tests does not depend on the distributional assumptions.\n\n-   One-sample data: consists of observations from a single population.\n\n-   Primary interest: make inference about the location/center of the single distribution.\n\n-   Two popular measurements of location/center\n\n    -   Mean: sensitive to outliers\n\n    -   Median: robust against outliers\n\n    -   Mean and Median are the same for symmetric distributions.\n\n## Parametric Methods\n\n### One-sample Z-test (see Chapter 0.3.1)\n\n-   Assumption: The random sample $X_1,...X_n$ are $i.i.d$ from $N(\\mu ,\\sigma^2)$, where $\\mu$ is the unknown mean, and $\\sigma^2$ is the variance and is known.\n-   Null hypothesis $H_0 : \\mu = \\mu_0$ (the distribution is centered at $\\mu_0$\\~, a prespecified null value).\n-   Significance level $\\alpha$, *i.e.* we want to control $$Type~I~error =  P(Reject~H_0~|~H_0~is~True) \\le \\alpha.$$\n-   **Test statistic** $$Z =\\frac{\\hat{X}-\\mu_0}{\\sigma / \\sqrt{n}}$$ where $\\hat{X}=\\dfrac{1}{n}{\\displaystyle \\sum_{i=i}^{n}X_i}$. Under $H_0,~ Z ∼ N(0,1).$\n\nUpper-tailed *Z*-test:\n\n$$H_\\alpha : \\mu > \\mu_0$$\n\n$$Rejection~Region = \\{z_{obs} : z_{obs} > z_{1-\\alpha}\\}$$\n\n::: {#tip-pvaluedef .callout-tip}\n#### P-value\n\n$$p \\text- value = P(Z > z_{obs})=1-\\Phi(z_{obs})$$\n:::\n\n$$Reject~H_0~if~z_{obs}~falls~into~the~RR~or~ p \\text- value < \\alpha.$$\n\n::: column-margin\n$z_\\alpha$: the $(1-\\alpha)$ th percentage point of $N(0, 1).$\n\nThe quantile $\\Phi^{-1}(\\alpha)$ of the standard normal distribution is commonly denoted as $z_{\\alpha}$.\n:::\n\nLower-tailed *Z*-test:\n\n$$H_\\alpha : \\mu < \\mu_0$$\n\n$$Rejection~Region = \\{z_{obs} : z_{obs} < z_\\alpha\\}$$\n\n$$p \\text- value = P(Z < z_{obs})=\\Phi(z_{obs})$$\n\nTwo-tailed *Z*-test:\n\n$$H_\\alpha : \\mu \\ne  \\mu_0$$\n\n$$Rejection~Region = \\{z_{obs} : |z_{obs}| > - z_{\\alpha/2}\\}$$\n\n$$p \\text- value = P(|Z| > |z_{obs}|)=2P(Z > |z_{obs}|)=2\\{1-\\Phi(|z_{obs}|)\\}$$\n\n### One-sample *t*-test\n\nSuppose $\\sigma ^2$ is unknown, but can be estimated by the sample variance\n\n$$S^2=(n-1)^{-1}\\sum_{i=1}^n(X_i-\\bar{X})^2.$$\n\n-   Test statistic: $$T=\\frac{\\bar{X}-\\mu_0}{S/\\sqrt{n}}.$$\n\n-   Under $H_0 : \\mu = \\mu_0, ~T ∼ t_{n−1}.$\n\n-   [Note that the test statistic does not follow $N(0, 1)$ as in the *Z*-test]{style=\"color:red;\"} , so the rejection region and *p*-value have to be calculated by using the *t* distribution table with *n* − 1 degrees of freedom instead of standard normal table.\n\n::: column-page-right\nT distribution calculator: [here](https://wise-jiang-kunyi.shinyapps.io/jky1/){target=\"_blank\"}\n\n\n```{=html}\n<script>\n  var appUrl = \"https://wise-jiang-kunyi.shinyapps.io/T-Stat/\";\n</script>\n\n<iframe height=\"800\" width=\"100%\" frameborder=\"yes\" src=\"\" id=\"appFrame\"> </iframe>\n\n<script>\n  document.getElementById(\"appFrame\").src = appUrl;\n</script>\n```\n\n:::\n\n### Large sample z-test\n\nSuppose $X_1,...X_n$ are $i.i.d$ with mean $\\mu$ and variance $\\sigma^2$ (both are unknown). Note here we do no assume normal distribution. Suppose *n* is large.\n\nTest statistic :\n\n$$T=\\frac{\\bar{X}-\\mu_0}{S/\\sqrt{n}}.$$ By CLT, under $H_0 : \\mu = \\mu_0, ~Z ∼ N(0，1)$ approximately for large *n*. So the hypothesis test can be carried out as in the *z*-test for normally distributed data.\n\n#### Example: IQ Test\n\n#### Example 1.1.1\n\nTen sampled students of 18-21 years of age received special training. They are given an IQ test that is $N(100, 10^2)$ in the general population. Let $\\mu$ be the mean IQ of these students who received special training. The observed IQ scores:\n\n121; 98; 95; 94; 102; 106; 112; 120; 108; 109\n\nTest if the special training improves the IQ score using significance level $\\alpha$= 0.05.\n\n1.  What is the rejection region? Calculate the *p*-value and state your conclusion.\n\n2.  What if the variance is unknown?\n\n::: {.callout-caution collapse=\"true\"}\n#### Answer\n\n::: {.border .p-3}\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(digits = 3)\nx <- c(121, 98, 95, 94, 102, 106, 112, 120, 108, 109);\nq <- 0.95\nn <- length(x)\nsort_x <- sort(x)\n```\n:::\n\n\nFirst we sort data: 94, 95, 98, 102, 106, 108, 109, 112, 120, 121\n:::\n\n::: {.border .p-3}\n\n::: {.cell}\n\n```{.r .cell-code}\nmean_x <- mean(x)\n```\n:::\n\n\nmean of data: 106.5\n:::\n\n::: {.border .p-3}\n\n::: {.cell}\n\n```{.r .cell-code}\nteststa <- (mean(x)-100)/(10/sqrt(n))\n```\n:::\n\n\nSince IQ is $N(100, 10^2)$, test statisitic is : 2.055\n:::\n\n::: {.border .p-3}\nand under $H_0: IQ=100$ with a significance level $\\alpha$= 0.05, the rejection region can be constructed as $$100\\pm z_{q}*10/\\sqrt n$$ that is:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(\"[\", 100 - qnorm(q)*10/sqrt(n), \",\", 100 + qnorm(q)*10/sqrt(n), \"]\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[ 94.8 , 105 ]\n```\n\n\n:::\n:::\n\n:::\n\n::: {.border .p-3}\nNow we can calculation the *p*-value by definition, see @tip-pvaluedef :\n\n\n::: {.cell}\n\n```{.r .cell-code}\npvalue1 <- 1-pnorm(teststa)\n```\n:::\n\n\nSince 0.02 $< 0.05$, conclude that reject $H_0$ with a significance level $\\alpha$= 0.05.\n:::\n\n------------------------------------------------------------------------\n\n------------------------------------------------------------------------\n\nTurn to unknown part : suppose sigma is unknown (one-sample t-test),\n\n\n::: {.cell}\n\n```{.r .cell-code}\ns <-  sd(x)\ntval <-  (mean(x) - 100)/(s/sqrt(n))\n```\n:::\n\n\ntest statisitic is 2.163\n\n::: {.border .p-3}\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- n-1\npvaluet <-  1-pt(tval, df)\n```\n:::\n\n\nSince *p*-value 0.029 \\< 0.05, conclude that reject $H_0$ with a significance level $\\alpha$ = 0.05, given unknown sigma.\n:::\n\n------------------------------------------------------------------------\n\n------------------------------------------------------------------------\n\nreference code :\n\n```{{r}}\n##\n#### R code: analysis of the IQ score data set\n##\n#Test if the mean score is significantly greater than 100\n#(1) suppose sigma=10 is known (z-test)\nx = c(121, 98, 95, 94, 102, 106, 112, 120, 108, 109);\nxbar = mean(x);\nsigma=10;\nn = length(x);\nzval = (xbar - 100)/(sigma/sqrt(n));\npvalue = 1-pnorm(zval)\n#(2) suppose sigma is unknown (one-sample t-test)\ns = sd(x)\ntval = (xbar - 100)/(s/sqrt(n))\ndf=n-1\npvalue = 1-pt(tval, df)\n#or use the existing R function, the default is 2-sided alternative\n# test if E(X)-100>0 (alternative)\nt.test(x-100, alternative =\"greater\");\n```\n:::\n\n### Type I Error & Power (Z-test)\n\n#### Example 1.1.2\n\nRefer to Example 1.1.1. Suppose $X_1,...X_n$ are $i.i.d$ from $N(\\mu ,\\sigma^2)$, where $\\sigma$ is known. Test $H_0 : \\mu = \\mu_0~ versus~ H_a : \\mu > \\mu_0$. Suppose the rejection region is: $X > 105.2$. Calculate the Type I error of this test procedure.\n\n$\\text{In general, for an upper-tailed z-test }H_0:\\mu=\\mu_0,\\:H_a:\\mu>\\mu_0.$\n\n$$\n\\text{For }RR=\\{\\bar{X}:\\bar{X}>C\\},\n$$\n\n$$\n\\text{Type I error}=P\\left(Z=\\frac{\\bar{X}-\\mu_0}{\\sigma/\\sqrt{n}}>\\frac{C-\\mu_0}{\\sigma/\\sqrt{n}}\\right)=1-\\Phi\\left(\\frac{C-\\mu_0}{\\sigma/\\sqrt{n}}\\right).\n$$\n\n::: {.callout-caution collapse=\"true\"}\n#### Answer\n\n$$\n\\text{Type I error}=P\\left(Z=\\frac{\\bar{X}-100}{10/\\sqrt{n}}>\\frac{105.4-100}{10/\\sqrt{n}}\\right)=1-\\Phi\\left(\\frac{105.4-100}{10/\\sqrt{n}}\\right).\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest2 <- (105.4 - 100)/(10/sqrt(n))\npvalue2 <- 1 - pnorm(test2)\n```\n:::\n\n\nso Type I error is $0.044$.\n:::\n\nNote: The above calculation holds approximately for the approximate z-test based on CLT when n is large.\n\n(Upper-tailed test) $H_0:\\mu=\\mu_0,~H_a:\\mu>\\mu_0.$ For a level $\\alpha$ z-test $RR=\\{\\bar{X}:\\bar{X}>\\mu_{0}+z_{1-\\alpha}*\\sigma/\\sqrt{n}\\}$\n\n$$\\text{Type I error}=P\\left(Z=\\frac{\\bar{X}-\\mu_0}{\\sigma/\\sqrt{n}}>z_{1-\\alpha}\\right)=1-\\Phi\\left(z_{1-\\alpha}\\right)=\\alpha. $$\n\nPower = 1-P(Type II error): the probability of detecting the departure from the null hypothesis, i.e. the chance of rejecting $H_0$ when $H_a$ is true. This depends on how far the true parameter is away from the null hypothesis. We often fix the parameter value under $H_a$.\n\n#### Example 1.1.3\n\nRefer to Example 1.1.1. Suppose the true mean is $\\mu$ = 105 (105 \\> $\\mu_0$ = 100 so $H_0$ is false). Derive the power of the z-test procedure (reject $H_0$ when $\\bar{X}$ \\> 105.2).\n\n(Upper-tailed Z-test) $H_0:\\mu=\\mu_0,~H_a:\\mu>\\mu_0,$ $RR=\\{\\bar{X}:\\bar{X}>C\\}.$ Suppose the true mean is $\\mu^{\\prime}>\\mu_0,$\n\n$$\\begin{aligned}\nPower(\\mu^{\\prime})~\n& =\\quad P(\\text{Reject }H_0|H_0\\text{ is false})  \\\\\n&=\\quad P(\\bar{X}>C|\\mu=\\mu') \\\\\n&=\\quad P\\left\\{Z=\\frac{\\bar{X}-\\mu^{\\prime}}{\\sigma/\\sqrt{n}}>\\frac{C-\\mu^{\\prime}}{\\sigma/\\sqrt{n}}\\right\\}=1-\\Phi\\left(\\frac{C-\\mu^{\\prime}}{\\sigma/\\sqrt{n}}\\right).\n\\end{aligned}$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreject_normal <- 100+qnorm(0.95)*10/sqrt(10)\ntest3 <- (reject_normal - 105)/(10/sqrt(n))\npower_normal <- 1 - pnorm(test3)\n```\n:::\n\n\nPower = 1-P(Type II error) = $0.475$\n\n### Confidence Interval\n\nSuppose $X_1,...X_n$ are $i.i.d$ from $N(\\mu ,\\sigma^2)$, where $\\sigma$ is known. Then (1-$\\alpha$) confidence interval for $\\mu$ is:\n\n$$[\\bar{X}-z_{1-\\frac{\\alpha}{2}}\\frac{\\sigma}{\\sqrt{n}},\\bar{X}+z_{1-\\frac{\\alpha}{2}}\\frac{\\sigma}{\\sqrt{n}}].$$\n\n#### Example 1.1.4\n\nFor the IQ test example, verify that the 95% confidence interval for $\\mu$ is:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(\"[\", mean(x) - qnorm(1-(1-q)/2)*10/sqrt(n), \",\", mean(x) + qnorm(1-(1-q)/2)*10/sqrt(n), \"]\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[ 100 , 113 ]\n```\n\n\n:::\n:::\n\n\nNote that it is not correct to say that P(100.3 \\< $\\mu$ \\< 112.7) = 0.95. Why? In general, $$P(\\bar{X}-z_{1-\\frac{\\alpha}{2}}\\frac{\\sigma}{\\sqrt{n}}<\\mu<\\bar{X}+z_{1-\\frac{\\alpha}{2}}\\frac{\\sigma}{\\sqrt{n}})=1-\\alpha$$\n\nwhere the lower and upper bounds are **random variables**. In the interval \\[100.3, 112.7\\], the values have been plugged in.\n\n## Binomial Test\n\n### Hypotheses and Test Statistic\n\n-   Assumption: suppose the random sample $X_1,...X_n$ are $i.i.d$ from a distribution with median $\\theta$. Different from Section 1.1, here we do not require the distribution to be Normal.\n\n-   Null hypothesis $H_0 : \\theta=\\theta_0$, where $\\theta_0$ is prespecified, e.g.$\\theta_0=0$ corresponds to testing if the distribution is centered at 0.\n\n-   Note that if the distribution is symmetric, the $H_0$ is equivalent to test if the population mean= $\\theta_0$.\n\n-   Alternative hypothesis $H_a : \\theta > \\theta_0$ (upper-tailed test).\n\nWe consider the binomial test statistic: $$S = number~ of~ X_is~ that~ exceed~ \\mu_0$$\n\nThat is, $S$ is the total number of observations out of n that exceed the hypothesized median $\\mu_0$. In another word,\n\n$$ S = \\sum_{i=1}^{n} I(X_i > \\mu_0);$$ where $I(A)$ is the indicator function that takes value 1 if the statement $A$ holds and zero otherwise.\n\nIn $S$, we care about only the signs of $X_i - \\mu_0$ (whether $X_i$ is greater than $\\mu_0$ or not), but not the magnitudes of $X_i - \\mu_0$. Therefore, the Binomial test is also called \"sign test\".\n\nTo determine the rejection region and calculate the *p*-value, we need know the distribution of the test statistic $S$ when $H_0$ is true.\n\n**Distribution of** $S$ under $H_0$: when $H_0$ is true, we would expect half of the data are greater than $\\theta_0$, and half are smaller than $\\theta_0$.\n\nTherefore,\n\n$$S ∼ Binomial(n, p = 0.5)~ under ~H_0;$$\n\nirrespective of the underlined distribution of $X_i's$. Here $p$ is the population proportion of $X_i > \\mu_0$. Thus, testing $H_0 : \\theta=\\theta_0$ versus $H_a : \\theta>\\theta_0$ is equivalent to testing\n\n$$H_0 : p = 0.5~ versus~ H_a : p > 0.5~.$$\n\n### Rejection Region, Type I Error and Power\n\n-   For the upper-tailed test, a larger value of $S$ provides more contradiction of $H_0$.\n\n-   Rejection region: reject $H_0$ when $S \\ge c_{val}$, where $c_{val}$ is the *critical value* that is determined to control Type I error at level $\\alpha$, that is, find $c_{val}$ such that $$P(S\\geq c_{val}\\,|H_0)=\\sum_{k=c_{val}}^n\\binom nk0.5^n=\\alpha.$$\n\n-   Since Binomial is a discrete distribution, we may not be able to find an integer $c_{val}$ to make the Type I error equal $\\alpha$ exactly. In practice, we find an integer $c_{val}$ such that the Type I error is as close to $\\alpha$ (no larger than) as possible.\n\nLarge Sample Approximation:\n\nFor large n, by Central Limit Theorem, we know that approximately\n\n$$S ∼ N(0.5n, 0.25n) ~under~ H_0.$$ Therefore, for large n, the approximate rejection region is:\n\n$$S ≥ 0.5n + z_{1-\\alpha}\\sqrt{0.25n},$$ which has Type I error approximately equal $\\alpha$.\n\n#### Example 1.2.1\n\nRefer to Example 1.1.1. The observed IQ scores:\n\n$$121; 98; 95; 94; 102; 106; 112; 120; 108; 109$$\n\nTest $H_0 : \\theta=100$ versus $H_a : \\theta>100$. Suppose the rejection region is: reject when $S ≥ 8$.\n\n1.  Calculate the Type I error of this test procedure.\n\n2.  Calculate the Type I error using the Normal approximation (based on CLT).\n\n::: {.callout-caution collapse=\"true\"}\n#### Answer\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntype.1.error <- sum(dbinom(8:10,10,0.5))\n```\n:::\n\n\nType I error: $$P(RR|H_0)=P(S \\ge 8|\\theta=100)=\\sum_{k=8}^{10}\\binom {k}{10}0.5^{10}=0.055$$\n\nUsing the Normal approximation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 10\nmean_s <- 0.5*n\nsd_s <- sqrt(0.5*(1-0.5)*n)\ntype.1.error.normal <- 1-pnorm((8-mean_s)/(sd_s))\n```\n:::\n\n\nType I error: $$P(RR|H_0)=P(S \\ge 8|\\theta=100)=P\\left(\\frac{S-5}{\\sqrt{2.5}} \\ge \\frac{8-5}{\\sqrt{2.5}} \\right)=0.029$$\n:::\n\nPower calculation: Suppose $X_1,...X_n$ are $i.i.d$ with median $\\theta$ and variance $\\sigma^2$.\n\n-   $H_0 : \\theta = \\theta_0~ versus~ H_a : \\theta > \\theta_0~.$\n\n-   Rejection region: $S ≥ c_{val}$.\n\n-   Suppose the true median is $\\theta' = \\theta_0$, what is the power of the test procedure?\n\n::: {#tip-p_prime .callout-tip}\n####### $p\\prime$\n\n-   Under $H_a : S ∼ Binomial(n, p')$, where\n\n$$p\\prime = P (X > \\theta_0)> 0.5~~.$$\n:::\n\n::: callout-note\nWe need know/assume the distribution of $X_i$ in order to obtain $p'$ and calculate the power.\n:::\n\nSuppose $p'$ is already calculated. Then the power of the sign test is:\n\n$$\\begin{aligned}\n\\beta(\\theta^{\\prime})& =\\quad P\\{S\\geq c_{val}|S\\sim Binomial(n,p')\\}  \\\\\n&=\\quad1-B(c_{val}-1;n,p^{\\prime})\\quad\\text{(exact, small sample)} \\\\\n&\\approx\\quad1-\\Phi\\left\\{\\frac{c_{val}-np^{\\prime}}{\\sqrt{np^{\\prime}(1-p^{\\prime})}}\\right\\}\\text{(large-sample)}.\n\\end{aligned}$$\n\n#### Example 1.2.2\n\nSuppose $X_1,...X_{10}$ are $i.i.d \\sim N(\\theta,10^2).$ Test $H_0 : \\theta = 100$ versus $H_a : \\theta > 100$. Consider two tests:\n\n(A) z-test: reject $H_0$ when $\\bar{X} >100 + 1.645 × 10/\\sqrt{10} = 105.2$ (Type I error: 0.05)\n\n(B) Binomial test: reject $H_0$ when $S ≥ 8$ (Type I error: 0.055)\n\nSuppose the truth is $\\theta= 105$. Compare the power of the two tests for detecting such a departure from $H_0$. (Refer to Example 1.1.3 for the power of the z-test.)\n\n::: {.callout-caution collapse=\"true\"}\n#### Answer\n\nFirst we calculate the $p^{\\prime}$: @tip-p_prime\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_prime <- 1-pnorm((100-105)/(10))\n```\n:::\n\n\n$p^{\\prime}=0.691$\n\nand the power of Binomial test:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npower_bi <- sum(dbinom(8:10,10,p_prime))\n```\n:::\n\n\nthe power of Binomial test $=0.36$, the power of z-test is $0.475$.\n:::\n\n#### Example 1.2.3\n\nSuppose $X_i$ are $i.i.d$ from the *Laplace* distribution with mean 105 and variance 100 with *pdf*\n\n$$f(x)=\\frac1{10\\sqrt{2}}\\exp\\left\\{-\\frac{|x-105|}{10/\\sqrt{2}}\\right\\},\\quad-\\infty<x<+\\infty.$$\n\nLaplace distribution is symmetric about the mean, but it has a fatter tail than the normal distribution. Find the power of the Binomial test.\n\n::: {.callout-caution collapse=\"true\"}\n#### Answer\n\nFirst we calculate the $p^{\\prime}$: @tip-p_prime\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages('VGAM')\nlibrary(VGAM); \np_prime_laplace <- 1 - plaplace(100,105, 10/sqrt(2))\n```\n:::\n\n\n$p^{\\prime}=0.753$\n\nand the power of Laplace test:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npower_bi_laplace <- sum(dbinom(8:10,10,p_prime_laplace))\n```\n:::\n\n\nthe power of Binomial test is $0.536$, larger than  z-test $(= 0.475)$.\n\nBinomial test is more powerful than z-test for heavy-tailed distributions!!\n:::\n\nSample size determination: For the large sample level $\\alpha$ sign test, $c_{val} = 0.5n + z_{1-\\alpha}\\sqrt{0.25n}.$ In order to achieve power $\\beta$, say, 90%, what is the smallest n required?\n\nWe need n such that\n\n$$\\beta(\\theta')\\quad\\approx\\quad1-\\Phi\\left\\{\\frac{c_{val}-np'}{\\sqrt{np'(1-p')}}\\right\\}\\geq\\beta.$$\n\nThat is, we need\n\n$$\\begin{aligned}\nz_{1-\\beta} &\\ge\\frac{0.5n+z_{1-\\alpha}\\sqrt{0.25n}-np^{\\prime}}{\\sqrt{np^{\\prime}(1-p^{\\prime})}} \\\\\n\\Rightarrow\\quad n&\\geq\\frac{\\{0.5z_{1-\\alpha}-z_{1-\\beta}\\sqrt{p^{\\prime}(1-p^{\\prime})}\\}^2}{(0.5-p^{\\prime})^2}\\text{ (round up).}\\end{aligned}$$\n\n#### Example 1.2.4 \n\n(Refer to Example 1.2.2) Desired power $\\beta = 0.475$, calculate the minimum sample size required for the Binomial test.\n\n::: {.callout-caution collapse=\"true\"}\n#### Answer\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta <- 0.475\nsize_require <- (0.5*qnorm(0.95)-qnorm(1-beta)*sqrt(p_prime*(1-p_prime)))**2/(0.5-p_prime)**2\n```\n:::\n\nThe minimum sample size required for the Binomial test is 18\n\n\n:::\n\nCalcuate the power of the two tests using simulation:\n\n::: {.cell}\n\n```{.r .cell-code}\nn=10\nreject.ztest = reject.binom = 0\nreject.ztest.laplace = reject.binom.laplace = 0\n\nfor(j in 1:1000){\nx = rnorm(n, 105, 10)\ny = rlaplace(n, location=105, scale=10/sqrt(2))\n\nxbar = mean(x)\nybar = mean(y)\n\nS.nor = sum(x>100)\nS.lap = sum(y>100)\n\nreject.ztest = reject.ztest + 1*(xbar > 105.2)\nreject.binom = reject.binom + 1*(S.nor>=8)\n\nreject.ztest.laplace = reject.ztest.laplace + 1*(ybar > 105.2)\nreject.binom.laplace = reject.binom.laplace + 1*(S.lap>=8)\n}\n```\n:::\n\nThe power of Binomial test is $0.395$, z-test is $0.517$.\n\nWhen $X_i$ follow $Laplace(105,10/\\sqrt{2})$ (variance is $100=scale^2$), the power of Binomial test is $0.566$, z-test is $0.487$.\n\n### Hypothesis Testing: *p*-value\n\n-   The *p*-value is the probability of obtaining a test statistic value as extreme as the observed value, calculated assuming $H_0$ is true.\n\n-   Suppose $Y \\sim Binomial(n, p)$, then the probability mass function of Y is:\n冲冲冲",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}